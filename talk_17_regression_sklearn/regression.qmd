---
title: Getting started with Regression using scikit-learn
author: Henrik Andersson
date: 2023-03-23
date-format: long
format: 
  revealjs:
    margin: 0.1
    slide-number: true
    theme: [default, assets/dhi_light.scss]
    footer: Getting started with Regression using scikit-learn

jupyter: python3
execute: 
  echo: true
  freeze: auto
---

## 
```{mermaid}
%%| echo: false
flowchart TB
  DA(Do you have data\n of the target variable?)
  SC(Scenarios?)
  ET(Predict rare events?)
  
  P(Known physics?)
  
  ML[Machine learning model]
  PB[Physics-based model]  
    
  DA --Yes--> SC
  DA --No--> P
  SC--No--> ET
  SC --Yes--> P
  ET --No--> ML
  ET --Yes--> P
  P --Yes --> PB
```


## Prediction with a rule-based model

```python
def model(height: float, direction: float) -> float:
  if direction < 90:
    return height * 0.8
  elif direction < 180:
    return height * 0.9
  elif direction < 270:
    return height * 0.7
  else:
    return height * 0.6
```

. . .

What if we want to include the wave period? ðŸ¤” 

## Prediction with a rule-based model

```python
def model(height:float, period:float, direction:float) -> float:
  
  if period < 5.0:
    if direction < 90:
      return height * 0.8
    ...
  else:
    if direction < 90:
      return height * 0.6
    ...
```

. . .

This is getting complicated. What if we want to include the wind speed? ðŸ˜Ÿ

## Machine learning vs rule-based models

```{mermaid}
%%| echo: false
flowchart LR
  Input --> Traditional
  Rules --> Traditional
  Traditional --> Output
```

. . .

```{mermaid}
%%| echo: false
flowchart LR
  Input --> ML[Machine Learning]
  Output --> ML
  ML --> Rules
```

## Types of Machine learning

```{mermaid}
%%| echo: false
flowchart TB
  C(Do you have labels?)
  C --No--> D(Unsupervised learning)
  C --Yes--> E(Supervised learning)
  E --> F(Are you predicting a\n continuous variable?)
  F --No --> H(Classification)
  F --Yes --> G(Regression)
```

## Workflow

```{mermaid}
%%| echo: false
flowchart TB
  DA(Data aquisition)  --> SP(Train/test split)
  SP --> DC(Data cleaning)
  DC --> AS(Algorithm selection)
  AS --> MT(Model training)
  MT --> ME(Skill assessment)
```


## ML lingo

::: {.incremental}
* **Feature**: A variable that is used to predict the target variable, also called an **independent** variable.
* **Feature vector**: A vector of features $\mathbf{X}$, also called a design matrix.
* **Target variable**: The variable that we want to predict $y$, also called a **dependent** variable.
::: 

. . .

* $\mathbf{X}$ is a matrix where each row is an example and each column is a feature.
* $y$ is a vector with one element for each example. 


##

```{python}
# | echo: false
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

```

```{python}
# | echo: false

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression


def true_fun(X):
    return np.cos(1.5 * np.pi * X)


np.random.seed(0)

n_samples = 29
degrees = [1, 4, 15]
titles = ["Underfitting", "Good fit", "Overfitting"]

X = np.sort(np.random.rand(n_samples))
y = true_fun(X) + np.random.randn(n_samples) * 0.1

with plt.xkcd():
    plt.figure(figsize=(14, 5))
    for i in range(len(degrees)):
        ax = plt.subplot(1, len(degrees), i + 1)
        plt.setp(ax, xticks=(), yticks=())

        polynomial_features = PolynomialFeatures(degree=degrees[i], include_bias=False)
        linear_regression = LinearRegression()
        pipeline = Pipeline(
            [
                ("polynomial_features", polynomial_features),
                ("linear_regression", linear_regression),
            ]
        )
        pipeline.fit(X[:, np.newaxis], y)

        X_test = np.linspace(0, 1, 100)
        plt.plot(X_test, pipeline.predict(X_test[:, np.newaxis]), label="Model")
        plt.scatter(X, y, edgecolor="b", s=20, label="Data")
        plt.xlabel("x")
        plt.ylabel("y")
        plt.xlim((0, 1))
        plt.ylim((-2, 2))
        plt.legend(loc="best")
        plt.title(titles[i])

plt.show()

```

::: {.notes}

You will be aware if you have an underfit model, since the skill be low.
An overfit model will have a high skill on the training data, but a low skill on the test data.
This is important to watch out for.

:::



## Train/test split

Splitting the data into a training and test set is a way to avoid overfitting.

For many datasets a **random** split is good and easy to do:

. . .

```{.python}
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
```


## Train/test split

For time series data, we should split the data chronologically, to avoid data leakage.

. . .

```{python}
# | echo: false

import numpy as np
import matplotlib.pyplot as plt

# Generate random time series data
x = np.arange(0, 50, 0.5)
y = np.sin(x) + np.random.normal(0, 0.2, size=len(x))

# Define the split point
split_point = int(len(x) * 0.7)

# Plot the entire time series
plt.subplots(figsize=(10, 5))
plt.plot(x, y, "k+-", label="Time series")

# Highlight the training section
plt.axvspan(-1, x[split_point], alpha=0.2, color="blue", label="Training section")

# Highlight the test section
plt.axvspan(x[split_point], 51, alpha=0.2, color="orange", label="Test section")

# Set chart title and axis labels
plt.title("Splitting a time series into training and test section")
plt.xlabel("Time")
plt.ylabel("Value")
plt.xlim(0, 50)

# Show the legend
plt.legend()

# Show the chart
plt.show()
```



```{python}
# | echo: false
df = pd.read_csv("data/wave.zip", index_col=0, parse_dates=True)
split_date = "2001-06-01"
train = df.loc[:split_date]
test = df.loc[split_date:]

y_train = train.pop("target")
X_train = train

y_test = test.pop("target")
X_test = test
```


## Standardization

Some regression methods are sensitive to the scale of the data.

* Neural networks
* K-nearest neighbors

A common way is to use z-score standardization.
The data is scaled to have zero mean and standard deviation of one.

$$ \frac{x - \mu}{\sigma} $$

## Standardization

```{python}
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaler.fit(X_train)
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```


## Use case: Downscaling wave height

![](assets/downscaling.png)

::: {.notes}
A local wave model has higher resolution, with a more accurate bathymetry and can resolve smaller features e.g. islands.

Can we train a machine learning to downscale the global wave model to the local wave model?
:::


## Baseline

Always start with a simple baseline.

::: {.incremental}

* What is the simplest model that can be used?
* Predicting the mean value of the target variable.
* For time series data, the previous value of the target variable.

:::


## Baseline


::: {.columns}


::: {.column}

In this case, the baseline is our global wave model.


:::


::: {.column}

```{python}
# | echo: false

from sklearn.metrics import r2_score

y_pred = X_test["Hs"]

r2 = r2_score(y_test, y_pred)


def plot_scatter(name, y_pred, r2, ax=None, test=True, legend=True):

    if ax is None:
        fig, ax = plt.subplots(figsize=(6, 6))

    if test:
        y = y_test
        label = "Test data"
    else:
        y = y_train
        label = "Train data"

    ax.scatter(y, y_pred, alpha=0.5, label=label)
    max_val = max(y.max(), y_pred.max())
    ax.plot([0, max_val], [0, max_val], "k--", label="1:1 line")
    ax.set_xlabel("Local wave model")
    ax.set_ylabel("Predicted")
    ax.set_title(f"{name} $R^2$ = {r2:.2f}")
    ax.set_xlim(0, max_val)
    ax.set_ylim(0, max_val)
    if legend:
        ax.legend()
    _ = ax.axis("square")


plot_scatter("Baseline", y_pred, r2)
```

:::
::::



## Regression

Regression is a supervised learning method where the target variable is continuous.

Some popular regression methods are:

* K-nearest neighbors
* Linear regression
* Gradient boosting


## K-nearest neighbors

Regression based on k-nearest neighbors.

The target is predicted by local interpolation of the targets associated of the nearest neighbors in the training set.

::: {.incremental}
* non-parametric, i.e. there are no parameters to learn, the model is just the training data
* fast to train, since no model is learned ðŸ¤¨
* can be slow to predict, since the distance to all training points must be calculated ðŸ¤”

:::


## K-nearest neighbors


```{.python code-line-numbers="1-3|4|6|8"}
from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor(n_neighbors=5) # Options depends on the model
knn.fit(X_train_scaled, y_train) # Fit the model

y_pred = knn.predict(X_train_scaled) # Make predictions

knn.score(X_test_scaled, y_test) # Score, for regression this is R^2
```

. . .

All models (*estimators*) in scikit-learn have:

* `fit` method to train the model
* `predict` method to make predictions
* `score` method to score the model


## K-nearest neighbors {.smaller}

:::: {.columns}

::: {.column}

```{python}
from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train_scaled, y_train)
y_pred = knn.predict(X_train_scaled)

knn.score(X_test_scaled, y_test)
```

:::

::: {.column}

```{python}
# | echo: false
y_pred_test = knn.predict(X_test_scaled)

plot_scatter("KNN", y_pred_test, knn.score(X_test_scaled, y_test))
```

:::
::::

## Linear regression

Linear regression can either be ordinary least squares or a regularized version, e.g. Ridge regression.

$$ \hat{y} = b_0 + b_1*x_1 + ... + b_n*x_n $$

. . .

$$ J_{ols} = \sum_{i=1}^n{(y_i - \hat{y}_i)^2}$$

. . .

$$ J_{ridge} = \sum_{i=1}^n{(y_i - \hat{y}_i)^2} + \alpha \sum_{j=1}^p w_j^2 $$

::: {.notes}

Bias (intercept) is not penalized.

:::


## Linear regression {.smaller}

:::: {.columns}

::: {.column}

```{python}
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1)
ridge.fit(X_train_scaled, y_train)
y_pred = ridge.predict(X_train_scaled)
```

:::

::: {.column}

```{python}
# |  echo: false
y_test_pred = ridge.predict(X_test_scaled)
plot_scatter("Ridge", y_test_pred, ridge.score(X_test_scaled, y_test))
```

:::
::::

## Polynomial regression {.smaller}

:::: {.columns}

::: {.column}

```{python}
from sklearn.preprocessing import PolynomialFeatures

poly = PolynomialFeatures(degree=2)
X_train_poly = poly.fit_transform(X_train_scaled)
X_test_poly = poly.transform(X_test_scaled)

ridge_poly = Ridge(alpha=1)
ridge_poly.fit(X_train_poly, y_train)
ridge_poly.score(X_test_poly, y_test)
```


:::

::: {.column}

```{python}
# |  echo: false
plot_scatter(
    "Polynomial", ridge_poly.predict(X_test_poly), ridge_poly.score(X_test_poly, y_test)
)
```

:::
::::

## Linear regression

Advantages of linear regression:

* Fast to train
* Easy to interpret
* Can be regularized
* Easy to run a trained model in any programming language


**C#**

```{.csharp filename="predict.cs"}
double y_pred = b0 + b1 * x1 + b2 * x2 + b3 * x3;
```


## Pipelines

```{mermaid}
%%| echo: false
flowchart LR

  scale[StandardScaler]
  poly[PolynomialFeatures]
  ridge[Ridge]
  scale --> poly
  poly --> ridge

```
. . .

```{.python code-line-numbers="|9|12"}
from sklearn.pipeline import make_pipeline

pipe = make_pipeline(
    StandardScaler(),
    PolynomialFeatures(degree=2),
    Ridge(alpha=1),
)

pipe.fit(X_train, y_train)
y_pred = pipe.predict(X_train)

y_pred_test = pipe.predict(X_test)

pipe.score(X_test, y_test)
```

::: {.notes}
Using pipelines avoids trivial mistakes such as applying the preprocessing steps to the test set.
:::


## Gradient boosting {.smaller}

:::: {.columns}

::: {.column}


* Gradient boosting is insensitive to the scale of the data.
* LightGBM is a separate library, compatible with scikit-learn.


```{python}
from lightgbm import LGBMRegressor

lgbm = LGBMRegressor(n_estimators=200)
lgbm.fit(X_train, y_train)

lgbm.score(X_test, y_test)
```

:::

::: {.column}

```{python}
# | echo: false
plot_scatter("Gradient boosting", lgbm.predict(X_test), lgbm.score(X_test, y_test))
```

:::
::::

::: {.notes}

Gradient boosting is a tree-based model.
It fits a weak learner (e.g. a decision tree) to the residuals of the previous model.

:::







## Timeseries

```{python}
# | echo: false
df = pd.DataFrame(
    {
        "global": X_test["Hs"],
        "local": y_test,
        "GB": lgbm.predict(X_test),
    }
)
df.head()
```

## Timeseries

```{python}
# | echo: false
_ = df["2001-07-09":"2001-07-25"].plot(style=["--", "k-", "r-"], figsize=(12, 6))
```


## Conclusion {.smaller}

::: {.incremental}

* scikit-learn is a great library for machine learning
* Functions for preprocessing, model selection, and evaluation
* There are many models to choose from
* All scikit-learn models have a similar API
* Pipelines are a great way to chain preprocessing steps and a model
* K-nearest neighbors is a simple, but powerful model for non-linear regression
* Linear regression can be powerful with the right preprocessing
* Gradient boosting is a powerful model for regression

:::


##

âœ¨ Thank you! âœ¨

## Comparison

```{python}
# | echo: false
fig, ax = plt.subplots(figsize=(18, 4), ncols=5, nrows=2, sharex=True, sharey=True)
plt.subplots_adjust(wspace=0.1, hspace=0.1)


y_pred_train = X_train["Hs"]

r2 = r2_score(y_train, y_pred_train)

plot_scatter(
    name="Baseline", y_pred=y_pred_train, r2=r2, ax=ax[0, 0], test=False, legend=False
)

plot_scatter(
    name="KNN",
    y_pred=knn.predict(X_train_scaled),
    r2=knn.score(X_train_scaled, y_train),
    ax=ax[0, 1],
    test=False,
    legend=False,
)
plot_scatter(
    name="Ridge",
    y_pred=ridge.predict(X_train_scaled),
    r2=ridge.score(X_train_scaled, y_train),
    ax=ax[0, 2],
    test=False,
    legend=False,
)
plot_scatter(
    name="Polynomial",
    y_pred=ridge_poly.predict(X_train_poly),
    r2=ridge_poly.score(X_train_poly, y_train),
    ax=ax[0, 3],
    test=False,
    legend=False,
)
plot_scatter(
    name="Gradient boosting",
    y_pred=lgbm.predict(X_train),
    r2=lgbm.score(X_train, y_train),
    ax=ax[0, 4],
    test=False,
    legend=False,
)

y_pred_test = X_test["Hs"]

r2 = r2_score(y_test, y_pred_test)

plot_scatter(name="Baseline", y_pred=y_pred_test, r2=r2, ax=ax[1, 0], legend=False)

plot_scatter(
    name="KNN",
    y_pred=knn.predict(X_test_scaled),
    r2=knn.score(X_test_scaled, y_test),
    ax=ax[1, 1],
    legend=False,
)
plot_scatter(
    name="Ridge",
    y_pred=ridge.predict(X_test_scaled),
    r2=ridge.score(X_test_scaled, y_test),
    ax=ax[1, 2],
    legend=False,
)
plot_scatter(
    name="Polynomial",
    y_pred=ridge_poly.predict(X_test_poly),
    r2=ridge_poly.score(X_test_poly, y_test),
    ax=ax[1, 3],
    legend=False,
)
plot_scatter(
    name="Gradient boosting",
    y_pred=lgbm.predict(X_test),
    r2=lgbm.score(X_test, y_test),
    ax=ax[1, 4],
    legend=False,
)
```





