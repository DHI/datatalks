---
title: Data Talk - Large Language Models
author: Jesper Mariegaard and Clemens Cremer
date: 2023-02-07
date-format: long
slide-number: c/t
transition: slide
format: revealjs
auto-stretch: false # do not resize figures to fit the slide
jupyter: python3
width: 1920
height: 1080
execute: 
  freeze: auto
  enable: true
---



# agenda

- start with chatGPT example (resume summary)
- general introduction LLM  
- examples chatgpt (record some in case it doesn't work)  
    - quick insight into some area (CAREFUL)  
	- assisted writing
	- reformatting table  
	- explain code
- example copilot  
	- quick functions
    - plotting and formatting (e.g. annotations) 

- where is this going   
- what does it mean for us?   


# AI Assistants
you probably use them every day

:::: columns
::: {.column width=50%}
- typing on your smartphone (autocomplete)
- search accuracy and relevance (google, bing,...)
- language translation (deepL, google translate,...)
- chatbots for customer service and support
- content creation (e.g. marketing material, product descriptions, blog posts,...)
- data analysis (extract insights from large amounts of text data) <font color=red> TRY WITH WATER LEVEL DATA! </font>
:::
::: {.column width=50%}
![](figures/autocomplete.png){width=100%}
:::
::::


# AI Assistants: Foundations  

- large language models (LLM)
- A type of generative AI model 
- Examples: GPT-3, BERT, T5, LaMDA, GPT-Neo, GPT-Codex
- process and generate natural language (e.g. text)
- also capable of text to image or other formats
- keeping track of state (conversational)
- working memory is limited 10-40k characters (you can't write a book with it in a go )
- can be very expensive to train


::: footer
https://www.youtube.com/watch?v=lnA9DMvHtfI
:::


## history
- are not new but originated 1950s - 1960s (e.g. ELIZA 1966 at MIT)
- got a boost with introduction of deep learning in 1990s and particularly
- unsupervised learning (no labels for unstructured data)







## broad overview 

:::: columns
::: {.column width=50%}
- **GPT**-like (Generative Pretrained Transformer) GPT-3.x or GPT-Neo, Dall-E (e.g. chatGPT, chatBCG) by **OpenAI**, >175 Billion parameters (GPT-4 expected to be >100x)
- **BERT** (Bidirectional Encoder Representations from Transformers) by **Google**, ~340 Million parameters
- **BART/T5**-like (Bidirectional Encoder Representations from Transformers, T5: Text-to-Text Transfer Transformer) by **Facebook**, ~400 Million parameters
- LaMDA (Language Model for Dialogue Applications) **Google** >135 bn parameters
- PaLM (Pretrained Autoregressive Language Model) **Google** > 540 bn parameters

**why are parameters important?**
- parameters related to sophistication of the model incl. handling tasks not being trained for (GPT2 could not do language translation while GPT3 can)
:::
::: {.column width=50%}
![Briefest history of transformers](figures/transformers_chrono_huggingface.svg)
![Parameters](figures/model_parameters.png)
:::
::::

::: footer
Links and sources: [1](https://www.omegavp.com/articles/introduction-to-large-language-models/), [2](https://huggingface.co/course/chapter1/4?fw=pt), [3](https://ai.googleblog.com/2022/01/lamda-towards-safe-grounded-and-high.html)
:::



# notes on using ChatGPT for code

- Explain code
- Improved existing code
- Rewrite code using correct style (refactor code made by non-native Python devs)
- Rewrite code using ideomatic constructs (make code Pythonic)
- Simplify code
- Exploring alternatives
- Writing documentation
- Writing tests
- Tracking down bugs



# github copilot (GPT-Codex)

- Codex: GPT3 fine tuned for programming tasks 
- translating natural language to code
- trained on GitHub codebase
- more context aware in your environment
- https://copilot.github.com/
- https://copilot.github.com/blog/introducing-copilot



# Where is it going?
- Wild west
- many things still unclear
- pros and cons as with every new technology


## Should we have **legal and ethical concerns**? 
 no, it can sort those out itself

![](figures/bar_exam.png){width=70%}

::: {.fragment}
...with a B to C grade though
:::

::: footer
[source](https://www.reuters.com/legal/transactional/chatgpt-passes-law-school-exams-despite-mediocre-performance-2023-01-25/)
:::

## Ok, probably we should consider some **open questions**

### Copyright  
**Art**: “If you train the AI to make Picasso-like works, or Mondrian-like works, and it makes one that is sufficiently similar, that could be a copyright infringement claim,” Mark Lemley, director of Stanford Law School’s Program in Law, Science and Technology

![](figures/dalle_picasso.png){height=70%, origin=center}

::: footer
[source](https://news.bloomberglaw.com/ip-law/wild-west-of-generative-ai-raises-novel-copyright-questions)
:::

## Open questions

### Copyright
**Music**: “If you train the AI to make music that sounds like a particular artist, and it makes a song that is sufficiently similar, that could be a copyright infringement claim,” Mark Lemley, director of Stanford Law School’s Program in Law, Science and Technology

::: footer
<font color="red"> this quote came from Copilot and cannot be found on the internet
</font>
:::

## Open questions

### Copyright
**Code and Open Source**: „Those duplications fail to include author attribution and licensing details, key elements of most open source software agreements, the plaintiffs said.“ from ongoing lawsuit against Copilot and OpenAI

![](figures/code.png){width=50%}

::: footer
[source](https://news.bloomberglaw.com/ip-law/wild-west-of-generative-ai-raises-novel-copyright-questions)
:::

## Open questions

### Security and privacy
ChatGPT is already used to build hacking tools at scale 

![](figures/hacking.png)

::: footer
[source](https://www.hackread.com/hackers-openai-chatgpt-malware/), [image](https://interestingengineering.com/culture/russian-hackers-chatgpt-malicious-code)
:::


## Open questions

### Ethical aspects and code of conduct

**Sophisticated bullshit and misinformation on steroids**

"Stack Overflow, the Q&A site for programmers, has banned ChatGPT-generated answers because **even its low-quality answers can be plausible-sounding.**" 

![](figures/guardian_misinformation.png){width="50%"}


::: footer
[source](https://www.zdnet.com/article/openai-is-hiring-developers-to-make-chatgpt-better-at-coding/)
:::

## Open questions
### Ethical aspects and code of conduct

**Hippocratic Oath**: Do no harm, respect privacy of patients

![](figures/doctor.png){width="30%"}


::: footer
[source](https://stanford-cs324.github.io/winter2022/lectures/legality/)
:::


# Ways forward

- should it be banned or restricted from education and elsewhere?
- can we cite or acknowledge sources „Written in Cooperation with AI?“


## positive perspectives by critical people 
- "I learned also learned writing by looking at other peoples work and getting inspired." Corey Doctorow     

- “Everybody loved Napster, but we all kind of knew it was illegal,” Butterick said. The digital **music streaming industry** that emerged later **did a better job at bringing artists and creators into the conversation, he added, and the AI industry should follow that model.**“ Butterick (filed a copilot lawsuit)

- "The bigger **potential for the profession here is that a lawyer could use ChatGPT to produce a rough first draft and just make their practice that much more effective**" Jonathan Choi (Prof. at Minnesota Law School, Evaluating ChatGPT performance on Bar exam)

::: footer
[source](https://podcasts.voxmedia.com/show/pivot), [source 2](https://www.reuters.com/legal/transactional/chatgpt-passes-law-school-exams-despite-mediocre-performance-2023-01-25/)
:::


# Am I useless now and what can I do?

	- "(Yann) LeCun (META Chief AI Scientist and Touring Award Winner) recently 
		- likened coding assistants such as Copilot to cruise control in cars. 
		- "Your hands need to remain on the wheel at all times," 
		- because **Copilot can generate errors in code with no awareness of the error**." [2](https://www.zdnet.com/article/openai-is-hiring-developers-to-make-chatgpt-better-at-coding/)
 


## potential (longer term)
- prohibitive cost (and environmental impact) to train LLM from scratch
- limited variety in LLMs will exist
- paid services in the works (e.g. ChatGPT Plus 20$/mo and ChatGPT API) [[1]](https://openai.com/blog/chatgpt-plus/)

- Chat GPT started a focused efford to improve code [[2]](https://www.zdnet.com/article/openai-is-hiring-developers-to-make-chatgpt-better-at-coding/)
- learning from other sources (images, audio recodings, videos, etc.)


- domain knowledge infused LLMs (build on pretrained LLMs)
- even this predictions might be false (e.g. transformer killer)

::: footer
[further reading](https://hai.stanford.edu/news/how-large-language-models-will-transform-science-society-and-ai)
:::

## what does it mean for me? (short term)


- you are using Assistants anyway. But if you haven't tried ChatGPT and Copilot --> try! It is free 
- adapt to leverage higher productivity  
- options apart from ChatGPT and copilot (YouChat, Jasper, and Chatsonic)

- prompt crafting: learn to ask the right questions or questions right

- if everyone can use it, everyone can do it --> basic programming skills no longer required?
- new skillsets needed?
	- new workflows and ways of working --> adopt workflows including LLMs 
	- unlearning old habits
	- critical mindset more important than ever --> bullshit does scale now
	- abitlity to review is important
- discussion...  


## For DHI engineers... 

- Text
    - Background information (be careful)  
    - Proposal and Report writing (insert application for job image here)
		- Differ in tone of voice,...
    - Presentations (slides)
		- try quarto
		- try chatBCG
- Code
    - Documentation    
    - Tests