---
title: Introduction to Artificial Neural Networks using PyTorch
author: Henrik Andersson
date: 2023-05-25
date-format: long
format: 
  revealjs:
    margin: 0.1
    slide-number: true
    theme: [default]
    footer: Introduction to Artificial Neural Networks using PyTorch
    incremental: true

jupyter: python3
execute: 
  echo: true
  freeze: auto
---

## Artificial Neural Networks

* Artificial Neural Networks (ANNs) are a class of machine learning models inspired by the human brain.
* ANNs are a class of models that can learn non-linear relationships between inputs and outputs.
* The number of trainable parameters in an ANN can be very large, and the models are therefore prone to overfitting.
* ANNs are often used for image classification, speech recognition, and natural language processing.

## Deep Learning

* Deep learning is a subfield of machine learning that uses ANNs to learn from data.
* A deep learning model is an ANN with many layers.
* Deep learning models are often trained using GPUs.
* Deep learning models are often trained using large amounts of data.

## Deep learning frameworks


* PyTorch is a Python library for deep learning. 
* Tensorflow is another popular Python library for deep learning.
* fast.ai is a high-level deep learning library built on top of PyTorch.

. . .

![](trends.png)

## Tensors

* Similar to NumPy arrays, PyTorch tensors are multi-dimensional arrays.
* PyTorch tensors can be used to store data and parameters in deep learning models.
* An important difference is that between NumPy arrays and PyTorch tensors is the ability to use GPUs for computation.

. . .

```{python}
import torch

x = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)
W = torch.randn(2, 3, dtype=torch.float32)

z = W @ x  # matrix multiplication
z
```

## Gradient descent

* Gradient descent is an optimization algorithm used to train deep learning models.
* The goal of gradient descent is to minimize a loss function.
* Finite differences can be used to approximate the gradient of a function, but this is very slow.
* The gradient of a function can be computed numerically using automatic differentiation.
* Manual differentiation is tedious and error-prone, and automatic differentiation is therefore used in deep learning frameworks.

## Stochastic gradient descent

* With many examples, computing the gradient of the loss function can be very slow.
* Stochastic gradient descent (SGD) uses a single randomly selected example to compute the gradient.
* This is much faster, but the gradient is noisy and the optimization can be unstable.
* A compromise between SGD and batch gradient descent is mini-batch gradient descent.


## Activation functions

* Activation functions are used to introduce non-linearities in deep learning models.
* Without activation functions, a deep learning model would be a linear model üòü
* For regression the activation function of the final layer is often the identity (*linear*) function.
* For binary classification the activation function is often the sigmoid function.


## Activation functions

```{python}
# | echo: false
import torch
import matplotlib.pyplot as plt

x = torch.linspace(-2, 2, 100)
y_relu = torch.relu(x)
y_sigmoid = torch.sigmoid(x)
y_tanh = torch.tanh(x)

plt.plot(x, y_relu, label="relu")
plt.plot(x, y_sigmoid, label="sigmoid")
plt.plot(x, y_tanh, label="tanh", linestyle="--")
_ = plt.legend()
```

::: {.notes}
A sigmoid function outputs values between 0 and 1, useful for binary classification.
:::


## Classification

![]https://www.oreilly.com/content/wp-content/uploads/sites/2/2020/01/Figure_1-71076f8ac360d6a065cf19c6923310d2.jpg) [^1]: O'Reilly: Screenshot from the ‚ÄúNot Hotdog‚Äù app courtesy of Ankur Desai.


## Classification

:::: {.columns}

::: {.column width="50%"}

* Satellite images can be used to classify land cover.
* Based on expert knowledge or ground truth I have classified pixels as two types of forest and farmland.
* The goal is to train a model that can classify pixels as forest or not forest based on the red, green, and blue values of the pixels.

:::

::: {.column width="50%"}

![](classification.png)

:::

::::


## Forest / not forest

```{python}
import pandas as pd

df = pd.read_csv("ground_truth.csv")
df.sample(5)
```

## Red, green, blue

```{python}
# | echo: false
import plotly.express as px

df["forest"] = df["class"] != 3

fig = px.scatter_3d(df, x="green", y="red", z="blue", color="forest")
fig.show()
```

## PyTorch

* Inputs need to be scaled to be in the range [-1, 1].
* By dividing by 255, the inputs are in the range [0, 1].

```{python}
import torch
from sklearn.model_selection import train_test_split

X = torch.tensor(df[["red", "green", "blue"]].values, dtype=torch.float32) / 255
y = torch.Tensor(df.forest).reshape(-1, 1)

X_train, X_test, y_train, y_test = train_test_split(X, y)
```

## A classifier

* 3 inputs (red, green, blue)
* ReLU activation function
* 1 hidden layer with 4 neurons
* 1 output (forest or not forest)
* Sigmoid activation function

. . .

```{python}
import torch.nn as nn

model = nn.Sequential(nn.Linear(3, 4), nn.ReLU(), nn.Linear(4, 1), nn.Sigmoid())
model
```

## Training the model
    
```{python}
# | echo: false
import torch.optim as optim

loss_fn = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

for epoch in range(10000):
    optimizer.zero_grad()
    y_pred = model(X_train)
    loss = loss_fn(y_pred, y_train)
    loss.backward()
    optimizer.step()
```

```{.python code-line-numbers="|3|4|6|7-12"}
import torch.optim as optim

loss_fn = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

for epoch in range(10000):
    optimizer.zero_grad()
    y_pred = model(X_train)
    loss = loss_fn(y_pred, y_train)
    loss.backward()
    optimizer.step()
```

. . .

* In this case we are using the full training set to compute the gradient.
* For larger datasets, this should be replaced by mini-batch gradient descent.

## A simple ANN for regression

* 1 input (x)
* Tanh activation function
* 1 hidden layer with 10 neurons
* 1 output (y)
* Linear activation function

```{python}

reg_model = nn.Sequential(nn.Linear(1, 10), nn.Tanh(), nn.Linear(10, 1))
reg_model

```

---

* The weights and biases of the model are initialized randomly.

```{python}
X = torch.tensor([[1], [2]], dtype=torch.float32)
y = reg_model(X)


```


## Regression model

```{python}
# | echo: false
def f(x):
    amplitude = 2 * torch.exp(-6 * x**2)
    period = 0.5

    return amplitude * torch.sin(2 * torch.pi * x / period) + 0.3


xx = torch.linspace(-1, 1, 100)[:, None]
yy = f(xx) + 0.1 * torch.randn(100, 1)
```


```{python}
# | echo: false
optimizer = optim.Adam(reg_model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

history = []

for i in range(10000):
    optimizer.zero_grad()
    y_pred = reg_model(xx)
    loss = loss_fn(y_pred, yy)
    loss.backward()
    optimizer.step()
    history.append(loss.item())
```

```{.python code-line-numbers="|2"}
optimizer = optim.Adam(reg_model.parameters(), lr=0.01)
loss_fn = nn.MSELoss()

for i in range(10000):
    optimizer.zero_grad()
    y_pred = reg_model(xx)
    loss = loss_fn(y_pred, yy)
    loss.backward()
    optimizer.step()
```

::: {.notes}
For regression problems, the loss function is usually the mean squared error (MSE). But PyTorch is extremely flexible and allows you to define your own loss functions.
:::

## Predictions

```{python}
# | echo: false
x_test = torch.linspace(-1.2, 1.2, 100)
y_pred = reg_model(x_test[:, None])
plt.plot(x_test, y_pred.detach().numpy(), label="Prediction", linewidth=1)
plt.scatter(xx, yy, label="Data", marker=".", alpha=0.5)
plt.ylim(-2, 3)

plt.legend()
```

# Save and load the model

To save the model in PyTorch native format:

```{python}
torch.save(reg_model, "model.pth")

predictor = torch.load("model.pth")
```

. . .

We can also save the model in [ONNX](https://onnx.ai/) format:

```{python}
torch.onnx.export(reg_model, xx, "model.onnx")
```

## Neural network architectures

* Feed-forward neural networks (FFNN)
* Convolutional neural networks (CNN)
* Recurrent neural networks (RNN)
* Transformers

::: {.notes}
This talk focuses on FFNNs. CNNs are used for image classification, RNNs for text and time series data, and transformers for text.
:::


## Summary

* **Tensors**: multi-dimensional arrays.
* **Activation function**: introduces non-linearity in the model.
* **Gradient descent**: optimization algorithm for finding the minimum of a function.
* Artificial neural networks (ANN) can do both classification and regression.
* For simple applications, you don't need special hardware to train deep learning models.
* For more complex applications, e.g. image classification, GPUs make a big difference.

## Playground

[Tensorflow Playground](https://playground.tensorflow.org/)