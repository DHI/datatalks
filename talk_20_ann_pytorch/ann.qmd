---
title: Introduction to Artificial Neural Networks using PyTorch
author: Henrik Andersson
date: 2023-05-25
date-format: long
format: 
  revealjs:
    margin: 0.1
    slide-number: true
    theme: [default]
    footer: Introduction to Artificial Neural Networks using PyTorch

jupyter: python3
execute: 
  echo: true
  freeze: auto
---

## Artificial Neural Networks

* Artificial Neural Networks (ANNs) are a class of machine learning models inspired by the human brain.
* ANNs are a class of models that can learn non-linear relationships between inputs and outputs.
* The number of trainable parameters in an ANN can be very large, and the models are therefore prone to overfitting.
* ANNs are often used for image classification, speech recognition, and natural language processing.

## Deep Learning

* Deep learning is a subfield of machine learning that uses ANNs to learn from data.
* A deep learning model is an ANN with many layers.
* Deep learning models are often trained using GPUs.
* Deep learning models are often trained using large amounts of data.

## PyTorch

* PyTorch is a Python library for deep learning.
* Tensorflow is another popular Python library for deep learning.

## Tensors

* Similar to NumPy arrays, PyTorch tensors are multi-dimensional arrays.
* PyTorch tensors can be used to store data and parameters in deep learning models.

```{python}
import torch

x = torch.tensor([1.0, 2.0, 3.0], dtype=torch.float32)
W = torch.randn(2, 3, dtype=torch.float32)

z = W @ x  # matrix multiplication
z
```

## Activation functions

* Activation functions are used to introduce non-linearities in deep learning models.
* Without activation functions, a deep learning model would be a linear model ğŸ˜Ÿ
* For regression the activation function of the final layer is often the identity (*linear*) function.
* For binary classification the activation function is often the sigmoid function.


```{python}
import torch
import matplotlib.pyplot as plt

x = torch.linspace(-2, 2, 100)
y_relu = torch.relu(x)
y_sigmoid = torch.sigmoid(x)
y_tanh = torch.tanh(x)

plt.plot(x, y_relu, label="relu")
plt.plot(x, y_sigmoid, label="sigmoid")
plt.plot(x, y_tanh, label="tanh", linestyle="--")
plt.legend()
```

```{note}
A sigmoid function outputs values between 0 and 1, useful for binary classification.
```

## {background-iframe="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.31132&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false&problem_hide=true&noise_hide=true&showTestData_hide=true&discretize_hide=true&resetButton_hide=false&regularization_hide=true&dataset_hide=false&batchSize_hide=true&playButton_hide=false&learningRate_hide=true&regularizationRate_hide=true&percTrainData_hide=true"}

## Replicate in PyTorch

```{python}
# | echo: false
from sklearn.datasets import make_circles, make_moons

X, y = make_moons(n_samples=500, noise=0.1)

X = X * 6.0

# train/test split
X_train, X_test = X[:400], X[400:]
y_train, y_test = y[:400], y[400:]

plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, marker="x")

X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32).reshape(-1, 1)
```




## 

```{python}
import torch
import torch.nn as nn
import torch.optim as optim

model = nn.Sequential(nn.Linear(2, 4), nn.Tanh(), nn.Linear(4, 1), nn.Sigmoid())


def loss_fn(y_pred, y):
    return -torch.mean(y * torch.log(y_pred) + (1 - y) * torch.log(1 - y_pred))


optimizer = optim.Adam(model.parameters(), lr=0.03)

for epoch in range(500):
    optimizer.zero_grad()
    y_pred = model(X_train)
    loss = loss_fn(y_pred, y_train)
    loss.backward()
    optimizer.step()
```

## 

```{python}
# predict the value at y=1.0 and x range -8 to 13
x = torch.ones(100, 2)
x[:, 0] = torch.linspace(-8, 13, 100)
y_pred = model(x).detach().numpy()

plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train)
# add a line at y=1.0
plt.axhline(1.0, linestyle="--", color="grey")
```
. . .

```{python}
plt.plot(x[:, 0], y_pred)
plt.axhline(0.5, linestyle="--", color="grey")
plt.scatter(X_train[:, 0], y_train, c=y_train)
```


## 












## A simple ANN for regression

1. \# of inputs = \# of features
2. \# of outputs = 1
3. \# of hidden layers = 1
4. \# of neurons in hidden layer = 10

```{python}
import torch
import torch.nn as nn


class FNN(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.l1 = nn.Linear(input_dim, 10)
        self.l2 = nn.Linear(10, 1)

    def forward(self, x):
        x = torch.tanh(self.l1(x))
        x = self.l2(x)
        return x


```

---

* The weights and biases of the model are initialized randomly.

```{python}
X = torch.tensor([[1], [2]], dtype=torch.float32)
model = FNN(input_dim=1)
y = model(X)

print(f"Shape of layer 1 weights: {model.l1.weight.shape}")
print(f"Shape of layer 2 weights: {model.l2.weight.shape}")
print("Total number of parameters:", sum(p.numel() for p in model.parameters()))
```


## Training a deep learning model with PyTorch

1. Define the model
2. Define the loss function
    * For regression: Mean Squared Error (MSE)
    * For classification: Cross Entropy Loss
3. The parameters are initialized randomly.
4. The PyTorch framework automatically computes the gradients of the loss function with respect to the parameters.
5. The gradients are used to update the parameters using gradient descent.

## Stochastic Gradient Descent

* TODO
* TODO


## Training a deep learning model with PyTorch

```{python}
import torch
import torch.nn as nn
import torch.optim as optim

optimizer = optim.SGD(model.parameters(), lr=0.01)  # or Adam
loss_fn = nn.MSELoss(reduction="mean")


# periodic function with amplitude fading away from zero
def f(x):
    amplitude = 2 * torch.exp(-6 * x**2)
    period = 0.5

    return amplitude * torch.sin(2 * torch.pi * x / period) + 0.3


xx = torch.linspace(-1, 1, 100)[:, None]
yy = f(xx) + 0.1 * torch.randn(100, 1)

# select 10 % of the data to try to overfit
xxs = xx[::10]
yys = yy[::10]

history = []

for i in range(10000):
    optimizer.zero_grad()
    y_pred = model(xxs)
    loss = loss_fn(y_pred, yys)
    loss.backward()
    optimizer.step()
    history.append(loss.item())
```


---

```{python}
plt.plot(history)
plt.xlabel("Epoch")
plt.ylabel("Loss")
```

```{python}
x_test = torch.linspace(-1.2, 1.2, 100)
y_pred = model(x_test[:, None])
plt.plot(x_test, y_pred.detach().numpy(), label="Prediction", linewidth=1)
plt.scatter(xx, yy, label="All data", marker=".", alpha=0.5)
plt.scatter(xxs, yys, label="Training data", marker="x", color="red")
# plt.plot(x_test, f(x_test), label="True function", linestyle="--")
plt.ylim(-2, 3)

# add a shaded area to indicate the training data
plt.fill_between(
    x_test,
    -2,
    3,
    where=(x_test > -1) & (x_test < 1),
    alpha=0.5,
    label="Interpolation region",
    color="beige",
)
plt.legend()
```

# Save and load the model

To save the model in PyTorch native format:

```{python}
torch.save(model, "model.pth")

predictor = torch.load("model.pth")
```

. . .

We can also save the model in [ONNX](https://onnx.ai/) format:

```{python}
torch.onnx.export(model, xx, "model.onnx")
```